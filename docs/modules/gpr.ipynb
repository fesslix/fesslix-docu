{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `gpr` - Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "### basic definitions\n",
    "\n",
    "Our goal is to learn function $f(\\mathbf{x}_\\star)$, where $\\mathbf{x}_\\star\\in\\mathbb{R}^m$ is a $m$ dimensional input vector. \n",
    "For learning, we have a total $n$ *noisy* input/output observations of $y(\\mathbf{x}_i) = f(\\mathbf{x}_i)+\\varepsilon_i$, with $i=1,\\ldots,n$ and $\\varepsilon_i$ is independent identically distributed noise with variance $\\sigma_\\varepsilon^2$.\n",
    "Let $\\mathbf{X}$ be the $n\\times m$ dimensional matrix of the observed input data-points; i.e., $\\mathbf{X}_i=\\mathbf{x}_{(i)}^\\operatorname{T}$ with $\\mathbf{x}_{(i)}$ as the $i$th observed input data-point of dimension $m$.\n",
    "$\\mathbf{y}$ is the $n$ dimensional vector of the observed *noisy* model output associated with $\\mathbf{X}$.\n",
    "\n",
    "Using Gaussian process regression, the prior of function $f(\\mathbf{x}_\\star)$ is expressed using the following surrogate model:\n",
    "\n",
    "$$f(\\mathbf{x}_\\star) = \\mathbf{b}(\\mathbf{x}_\\star)^\\operatorname{T}\\mathbf{a}+\\mathbf{Z}(\\mathbf{x}_\\star)\\;,$$\n",
    "\n",
    "where $\\mathbf{b}(\\mathbf{x}_\\star)^\\operatorname{T}\\mathbf{a}$ is a trend function that depends on a $k$ dimensional parameter vector $\\mathbf{a}$ and vector of shape functions $\\mathbf{b}(\\cdot)$. \n",
    "$\\mathbf{Z}(\\mathbf{x}_\\star)$ is a zero mean Gaussian process with variance $\\sigma_Z^2$ and autocorrelation coefficient function $r(\\mathbf{x},\\mathbf{x}^\\prime)$.\n",
    "\n",
    "### joint distribution model\n",
    "\n",
    "The joint distribution of the observed target values and the function value at $\\mathbf{x}_\\star$ is:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{matrix}\n",
    "        \\mathbf{y}\\\\\n",
    "        f_\\star\n",
    "    \\end{matrix}\n",
    "\\right] =\n",
    "\\mathcal{N}\\left(\n",
    "    \\left[\n",
    "    \\begin{matrix}\n",
    "        \\mathbf{B}\\mathbf{a}\\\\\n",
    "        \\mathbf{b}_\\star^\\operatorname{T}\\mathbf{a}\n",
    "      \\end{matrix}\n",
    "    \\right]\n",
    ",\n",
    "    \\left[\n",
    "    \\begin{matrix}\n",
    "        \\sigma_Z^2\\mathbf{R}+\\sigma_\\varepsilon^2\\mathbf{I} & \\mathbf{k}_\\star \\\\\n",
    "        \\mathbf{k}_\\star^\\operatorname{T} & k_{\\star\\star}\n",
    "      \\end{matrix}\n",
    "    \\right]\n",
    "\\right)\\;,\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "- $\\mathbf{y}$ is the $n$ dimensional vector of the observed *noisy* model output.\n",
    "- $f_\\star$ is the function value at $\\mathbf{x}_\\star$ that is to be predicted,\n",
    "- $\\mathbf{b}_\\star = \\mathbf{b}(\\mathbf{x}_\\star)$ is the vector of shape functions evaluated at $\\mathbf{x}_\\star$,\n",
    "- $\\mathbf{B}_{ij} = \\mathbf{b}_j(\\mathbf{x}_{(i)})$; i.e, $\\mathbf{B}$ is a $n\\times k$ dimensional matrix with coefficients as the $j$th shape function evaluated at $\\mathbf{x}_{(i)}$,\n",
    "- $\\mathbf{R}_{ij} = r\\left(\\mathbf{x}_{(i)},\\mathbf{x}_{(j)}\\right)$,\n",
    "- $\\left(\\mathbf{k}_{\\star}\\right)_j = \\sigma_Z^2 \\cdot r\\left(\\mathbf{x}_{(j)},\\mathbf{x}_{\\star}\\right)$, and\n",
    "- $k_{\\star\\star} = \\sigma_Z^2 \\cdot r\\left(\\mathbf{x}_{\\star},\\mathbf{x}_{\\star}\\right) = \\sigma_Z^2$.\n",
    "\n",
    "\n",
    "### predictive distribution\n",
    "\n",
    "$$\n",
    "\\operatorname{E}\\left[f_\\star\\right] = \n",
    "    \\mathbf{b}_\\star^\\operatorname{T}\\mathbf{a}\n",
    "    + \\mathbf{k}_\\star^\\operatorname{T}\\left(\\sigma_Z^2\\mathbf{R}+\\sigma_\\varepsilon^2\\mathbf{I}\\right)^{-1}\\left(y-\\mathbf{B}\\mathbf{a}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}\\left[f_\\star\\right] = \n",
    "    k_{\\star\\star} \n",
    "    - \\mathbf{k}_\\star^\\operatorname{T}\\left(\\sigma_Z^2\\mathbf{R}+\\sigma_\\varepsilon^2\\mathbf{I}\\right)^{-1} \\mathbf{k}_\\star\n",
    "$$\n",
    "\n",
    "### log marginal likelihood\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ln(\\mathcal{L}) = \n",
    "     &-\\frac{1}{2} \\left(y-\\mathbf{B}\\mathbf{a}\\right)^\\operatorname{T}\\left(\\sigma_Z^2\\mathbf{R}+\\sigma_\\varepsilon^2\\mathbf{I}\\right)^{-1}\\left(y-\\mathbf{B}\\mathbf{a}\\right) \\\\\n",
    "     &- \\frac{1}{2} \\log\\left|\\sigma_Z^2\\mathbf{R}+\\sigma_\\varepsilon^2\\mathbf{I}\\right| \\\\\n",
    "     &- \\frac{n}{2} \\log\\left(2\\pi\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### least squares estimation\n",
    "\n",
    "#### introduction\n",
    "Note that the covariance of the *noisy* model output can be re-written as:\n",
    "\n",
    "$$\n",
    "\\sigma_Z^2\\mathbf{R}+\\sigma_\\varepsilon^2\\mathbf{I} \n",
    "    = \\sigma_Y^2 \\left( \\sigma_Z^2/\\sigma_Y^2\\mathbf{R} + \\sigma_\\varepsilon^2/\\sigma_Y^2\\mathbf{I} \\right) \n",
    "    = \\sigma_Y^2 \\mathbf{Q} \\;,\n",
    "$$\n",
    "\n",
    "with $\\sigma_Y^2 = \\sigma_Z^2+\\sigma_\\varepsilon^2$.\n",
    "\n",
    "Thus, the predictive distribution can also be stated as:\n",
    "\n",
    "$$\n",
    "\\operatorname{E}\\left[f_\\star\\right] = \n",
    "    \\mathbf{b}_\\star^\\operatorname{T}\\mathbf{a}\n",
    "    + \\mathbf{k}_\\star^\\operatorname{T}\\left(\\sigma_Y^2\\mathbf{Q}\\right)^{-1}\\left(y-\\mathbf{B}\\mathbf{a}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}\\left[f_\\star\\right] = \n",
    "    k_{\\star\\star} \n",
    "    - \\mathbf{k}_\\star^\\operatorname{T}\\left(\\sigma_Y^2\\mathbf{Q}\\right)^{-1} \\mathbf{k}_\\star\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\ln(\\mathcal{L}) &=\n",
    "       &-\\frac{1}{2} \\left(y-\\mathbf{B}\\mathbf{a}\\right)^\\operatorname{T}\\left(\\sigma_Y^2\\mathbf{Q}\\right)^{-1}\\left(y-\\mathbf{B}\\mathbf{a}\\right) \\\\\n",
    "     & &- \\frac{1}{2} \\log\\left|\\sigma_Y^2\\mathbf{Q}\\right|  \\\\\n",
    "     & &- \\frac{n}{2} \\log\\left(2\\pi\\right) \\\\\n",
    "     &= \n",
    "       &-\\frac{1}{2\\sigma_Y^2} \\left(y-\\mathbf{B}\\mathbf{a}\\right)^\\operatorname{T}\\mathbf{Q}^{-1}\\left(y-\\mathbf{B}\\mathbf{a}\\right) \\\\\n",
    "     & &- \\frac{n}{2} \\log\\left(\\sigma_Y^2\\right) - \\frac{1}{2} \\log\\left|\\mathbf{Q}\\right| \\\\\n",
    "     & &- \\frac{n}{2} \\log\\left(2\\pi\\right)     \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "#### least squares estimation of the trend function\n",
    "\n",
    "It can be shown that conditional on $\\mathbf{Q}$, the vector $\\hat{\\mathbf{a}}$ that maximizes the likelihoood function is:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{a}} = \\left(\\mathbf{B}^\\operatorname{T}\\mathbf{Q}^{-1}\\mathbf{B}\\right)^{-1} \\mathbf{B}^\\operatorname{T}\\mathbf{Q}^{-1}\\mathbf{y}\n",
    "$$\n",
    "\n",
    "#### least squares estimation of $\\sigma_Y^2$\n",
    "\n",
    "It can be shown that conditional on $\\mathbf{Q}$ and $\\mathbf{a}$, the $\\hat{\\sigma}_Y^2$ that maximizes the likelihoood function is:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_Y^2 \n",
    "    = \\frac{1}{n} \n",
    "    \\left(y-\\mathbf{B}\\mathbf{a}\\right)^\\operatorname{T}\\mathbf{Q}^{-1}\\left(y-\\mathbf{B}\\mathbf{a}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    ".. class:: flx.gpr.gp\n",
    "\n",
    "   Represents/manages a Gaussian process.\n",
    "\n",
    "   A Gaussian process is completely defined by its mean function (*trend*) and autocovariance function (*kernel*). \n",
    "   For Gaussian Process Regression (GPR), the process is conditioned on data (of the model input and output).\n",
    "   The functions representing the *trend* and the *kernel* can utilize so-called *process parameters*. \n",
    "   The values of these parameters are optimized such that the likelihood of observing the provided data is maximized.\n",
    "   \n",
    "\n",
    "   .. method:: __init__(config)\n",
    "\n",
    "      Defines the (unconditional) Gaussian process.\n",
    "      \n",
    "      :param config: Configuration directory. The structure of ``config`` is outlined in detail in the following.\n",
    "      :type config: dict\n",
    "\n",
    "      General properties:\n",
    "         The following properties are required/allowed in *config*, independent of the type of the process.\n",
    "         \n",
    "         - ``name`` (:type:`Word`, default:\"name_unspecified\"): The name to assign to the process.\n",
    "         - ``Ndim`` (*int*): The dimension of the Gaussian process; i.e., the number of variables for the model input. Value must be a positive integer.\n",
    "         - ``type`` (:type:`Word`, default:``\"singleGP\"``): The type of the process. Can either be set to ``\"singleGP\"`` or ``\"mavgGP\"``. Properties associated with the different types are documented in the following.\n",
    "         - ``useLSE`` (*bool*, default:*True*): \n",
    "         \n",
    "             If set to *True*, the *process parameters* associated with the function for the mean value and standard deviation are directly identified \n",
    "             (conditional on the other process parameters) using *least squares*. \n",
    "             The remaining *Ndim* process parameters for the correlation length can be identified through a numerical optimization (see :func:`flx.gpr.gp.optimize`). \n",
    "             If set to *False*, all *process parameters* need to be identified through numerical optimization (see :func:`flx.gpr.gp.optimize`).\n",
    "\n",
    "         - ``descr`` (*str*, default:\"\"): An arbitrary description for the process.\n",
    "          \n",
    "      Processes of type ``\"singleGP\"``:\n",
    "         ... represent a \"standard\" Gaussian process. The following properties are required/allowed in *config*:\n",
    "         \n",
    "         - ``mean_type`` (:type:`Word`, default:\"zero\"): The type of the function used to represent the *trend*. The following options are allowed:\n",
    "\n",
    "             - ``\"zero\"``: The trend is set to *zero*. No *process parameters* are registered.\n",
    "             - ``\"const\"``: The trend is set to a constant value. No *process parameters* are registered. The constant value is specified by means of configuration option ``mean_value`` that expects a value of type *float*.\n",
    "             - ``\"universal\"``: The trend is represented as a parametrized function based on multivariate polynomials. The order of the polynomial is set using the configuration option ``mean_polyo`` that expects ``0``, ``1``, ``2`` or ``3`` as integer values. The parameters of the so-obtained polynomial are registered as *process parameters*. By default, the first process parameter (the intercept) is *one* and all other process parameters are set to *zero*.\n",
    "             - ``\"ordinary\"``: A user-defined function is assigned as trend function. For this type of trend function, a single *process parameter* is registered. It is *one* by default and scales the specified user-defined function of the trend. The user-defined function is specified by means of configuration option ``mean_fun`` that expects an expression of type :type:`flxParaFun` (which accepts a *Ndim*-dimensional input-array).\n",
    "         \n",
    "         For the covariance *kernel*, standard kernel types can be specified separately for each dimension:\n",
    "         \n",
    "         - ``kernel_lst`` (*list* of *str*): A list that specifies the kernel type associated with each dimension. The size of the list must match *Ndim*. The following types are supported:\n",
    "\n",
    "             - ``gauss``: Guassian kernel\n",
    "\n",
    "                 :math:`\\exp\\left[-\\left(\\frac{x_1-x_2}{\\lambda}\\right)^2\\right]`\n",
    "             \n",
    "             - ``exp``: exponential kernel\n",
    "\n",
    "                 :math:`\\exp\\left[-\\frac{\\left|x_1-x_2\\right|}{\\lambda}\\right]`\n",
    "\n",
    "             In the equations above, :math:`\\lambda` specifies the correlation length.\n",
    "\n",
    "             For this type of kernel function, ``Ndim+1`` *process parameters* are registered. The first controls the standard deviation of the Gaussian process, the other *Ndim* *process parameters* control the correlation length associated with each of the *Ndim*-kernels.\n",
    "\n",
    "         Alternatively, a user-defined function can be specified as *kernel*:\n",
    "\n",
    "         - ``kernel_fun`` (:type:`flxParaFun`): The user-defined covariance kernel function, which accepts a *2xNdim*-dimensional input array.\n",
    "             \n",
    "             For example, if ``Ndim=3``, the interpretation of the input array is ``[X11, X12, X13, X21, X22, X23]``. \n",
    "             \n",
    "             For this type of kernel function, no *process parameters* are registered.\n",
    "\n",
    "         \n",
    "      Processes of type ``\"mavgGP\"``:\n",
    "         ... represent an umbrella-class that contains a set of Gaussian processes of type ``\"singleGP\"``. For the final prediction, Bayesian model averaging is used to average over multiple Gaussian process models.\n",
    "         \n",
    "         - ``itermax`` (*int*, default: 500): The maximum number of iterations in the MLE optimization of each process model contained in the set (compare TODO[optimize]).\n",
    "         \n",
    "             The optimization of the individual underlying process models is performed implicitly whenever the parent object is conditioned on new data. \n",
    "             In this regard, the parent object differs from the object of a single process model (of type ``\"singleGP\"``) that is only optimized when explicitly requested by the user.\n",
    "\n",
    "      \n",
    "   .. py:method:: get_name()\n",
    "\n",
    "      Retrieves the name of the Gaussian process.\n",
    "\n",
    "      :returns: name of the Gaussian process\n",
    "      :rtype: str\n",
    "      \n",
    "   .. py:method:: get_descr()\n",
    "\n",
    "      Retrieves the description of the Gaussian process.\n",
    "\n",
    "      :returns: description of the Gaussian process\n",
    "      :rtype: str\n",
    "      \n",
    "   .. py:method:: noise_white(noise_sd)\n",
    "\n",
    "      Specifies the standard deviation of the model error of the Gaussian process model.\n",
    "      Set this to a value larger than *zero* if the Gaussian process model cannot interpolate the observed values of the model output exactly (due to uncertainty/noise).\n",
    "\n",
    "      :param noise_sd: Standard deviation of white noise. Value must be positive or *zero*.\n",
    "      :type noise_sd: float\n",
    "      :rtype: None\n",
    "\n",
    "   .. py:method:: condition_on(data_input, data_output, init_pvec=True, opt_noise=True)\n",
    "\n",
    "      Condition a Gaussian process on input/output-data.\n",
    "\n",
    "      :param data_input: Specifies *N* data-points of the model input.\n",
    "      :type data_input: numpy.ndarray  of shape (Ndim, N)\n",
    "      :param data_output: Specifies the model output associated with the *N* data-points of the model input.\n",
    "      :type data_output: numpy.ndarray  of shape (N,)\n",
    "      :param init_pvec: If set to *True*, the *process parameters* are initialized using a heuristic based on the statistics of the provided data.\n",
    "      \n",
    "          - If the mean function is of type ``\"ordinary\"`` the specified function is scaled with the sample mean of the specified model output data (only if ``useLSE=False``).\n",
    "          \n",
    "          - If the mean function is of type ``\"universal\"``, the entire (polynomial-based) mean function is scaled with the sample mean of the specified model output data (only if ``useLSE=False``). The *process parameter* that controls the intercept is set to *one* and all other mean-related *process parameters* are set to *zero*.\n",
    "\n",
    "          - The *process parameter* that controls the standard deviation is scaled using the sample standard deviation of the specified model output data. \n",
    "\n",
    "          - The *process parameters* that control the correlation length of the *Ndim* kernels are scaled using the sample standard deviation of the corresponding model input.\n",
    "\n",
    "          - Also an initial value for the standard deviation of *noise* is set (although it is not considered a standard *process parameter*). The value is selected as ``f*(sample standard deviation of the specified model output data)``, where *f* is ``0.1`` if ``opt_noise==True`` and ``1e-4`` otherwise.\n",
    "\n",
    "      :type init_pvec: bool\n",
    "      :param opt_noise: If set to *True*, the optimal standard deviation of the (white) noise associated with the process is identified through numerical optimization (the existing/old value is overwritten).\n",
    "\n",
    "          A log of the optimization of the standard deviation of the *noise* is stored internally.\n",
    "          The logged messages of the optimization can be accessed using :func:`flx.gpr.gp.info`.\n",
    "      \n",
    "      :type opt_noise: bool\n",
    "      :return: log-likelihood associated with the current values of the *process parameters*.\n",
    "      :rtype: float\n",
    "\n",
    "      The data in ``data_input`` and ``data_output`` is not copied and stored separately by the gp-model, \n",
    "      but only a pointer to the data is stored as part of the gp-model. \n",
    "      Therefore, one should be careful not to modify the content of the arrays associated with ``data_input`` and ``data_output``\n",
    "      after the gp-model was conditioned on the data. \n",
    "      If the data associated with ``data_input`` and ``data_output`` is modified, the method :func:`flx.gpr.gp.unassemble` must be called.\n",
    "\n",
    "\n",
    "   .. py:method:: optimize(itermax=500, opt_noise=False)\n",
    "\n",
    "      Optimizes the *process parameters* of the Gaussian process model.\n",
    "      \n",
    "      :param itermax: The maximum number of iterations during MLE of the *process parameters* of the Gaussian process.\n",
    "      :type itermax: int\n",
    "      :param opt_noise: \n",
    "          - ``True``: noise parameter is optimized.\n",
    "          - ``False``: The standard deviation of the *white noise* associated with the Gaussian process model is not interpreted as a *process parameter* and, therefore, not optimized.  \n",
    "            To find the optimal noise for the current values of the *process parameters*, \n",
    "            you can re-run :func:`flx.gpr.gp.condition_on` with ``init_pvec=True`` and ``opt_noise=True`` afterwards.\n",
    "      :type opt_noise: bool\n",
    "\n",
    "      A log of the optimization is stored internally.\n",
    "      The logged messages of the optimization can be accessed using :func:`flx.gpr.gp.info`.\n",
    "\n",
    "      .. important::\n",
    "    \n",
    "            Only process models of type ``\"singleGP\"`` are optimized by this function call. \n",
    "            Process models of type ``\"mavgGP\"`` are optimized automatically whenever the umbrella process is condition on new data (by means of :func:`flx.gpr.gp.condition_on`).\n",
    "      \n",
    "      :param opt_noise: If *True*, the standard deviation of the *noise* is treated as a *process parameter* and optimized.\n",
    "      :type opt_noise: bool\n",
    "      :return: log-likelihood associated with the current values of the *process parameters*.\n",
    "      :rtype: float\n",
    "      \n",
    "   .. py:method:: predict(x_vec, type, predict_noise=False)\n",
    "\n",
    "      Evaluate/predict quantities of the gp-model conditioned on the observations.\n",
    "\n",
    "      The behavior of the method depends on the value of ``type``:\n",
    "\n",
    "          - ``\"mean\"``: The mean value of the prediction at ``x_vec`` is returned. (*float*)\n",
    "          - ``\"sd\"``: The standard deviation of the prediction at ``x_vec`` is returned. (*float*)\n",
    "          - ``\"mean_sd\"``: A tuple of the mean value and the standard deviation of the prediction at ``x_vec`` is returned. (*float*, *float*)\n",
    "          - ``\"var\"``: The variance of the prediction at ``x_vec`` is returned. (*float*)\n",
    "          - ``\"trend\"``: The (prior) value of the trend function at ``x_vec`` is returned. (*float*)\n",
    "          - ``\"kernel\"``: The (prior) value of the kernel evaluated at ``x_vec`` is returned. Note that the dimension of ``x_vec`` must equal ``2*N_dim``. The expected structure of ``x_vec`` equals the one documented in :func:`flx.gpr.gp.__init__`, for parameter ``kernel_fun``. (*float*)\n",
    "      \n",
    "      :rtype: Depends on ``predict_noise``\n",
    "      \n",
    "   .. py:method:: info()\n",
    "\n",
    "      Returns a *dict* containing information about the Gaussian process.\n",
    "\n",
    "      The returned *dict* has the following entries:\n",
    "\n",
    "          - ``type`` (*str*): type of the Gaussian process\n",
    "          - ``name`` (*str*): name of the Gaussian process\n",
    "          - ``descr`` (*str*): description of the Gaussian process\n",
    "          - ``mean`` (*dict*): information about the trend function of the Gaussian process\n",
    "\n",
    "              - ``type`` (*str*): type of the trend function\n",
    "\n",
    "              Type ``\"const\"`` exports:\n",
    "\n",
    "                  - ``val`` (*float*): Constant value that represents the trend function.\n",
    "                  \n",
    "              Type ``\"ordinary\"`` exports:\n",
    "\n",
    "                  - ``para_vec`` (*numpy.ndarray*): Array of *process parameters* associated with the trend function.\n",
    "\n",
    "              Type ``\"universal\"`` exports:\n",
    "\n",
    "                  - ``para_vec`` (*numpy.ndarray*): Array of *process parameters* associated with the trend function.\n",
    "                  - ``normalizef`` (*float*): Scaling constant for the trend function.\n",
    "\n",
    "          - ``kernel`` (*dict*): information about the (prior) kernel of the Gaussian process\n",
    "\n",
    "              - ``type`` (*str* or *list*): type of the kernel\n",
    "\n",
    "              If a list of standard kernels is used, the following properties are additionally exported:\n",
    "\n",
    "                  - ``para_vec`` (*numpy.ndarray*): Array of *process parameters* associated with the kernel.\n",
    "                  - ``n_vec`` (*numpy.ndarray*): Array of scaling values linked to the *process parameters*.\n",
    "                  - ``kernel_sd`` (*float*): Standard deviation of the kernel.\n",
    "                  \n",
    "          - ``noise`` (*float*): standard deviation of *white noise*\n",
    "          - ``noise_log`` (*str*): Logging messages from the optimization of the standard deviation of *noise*.\n",
    "          - ``opt_log`` (*str*): Logging messages from the optimization of the *process parameters*.\n",
    "          - ``logl_obsv`` (*float*): Log-likelihood of the current observation conditional on the *process parameters*. This entry is only generated if the process is conditioned on a data set.\n",
    "          - ``N_obsv`` (*int*): Total number of registered observations.\n",
    "          - ``obsv_up2date`` (*bool*): *True*, if the data-set is considered to be \"up to date\".\n",
    "\n",
    "      :rtype: dict\n",
    "\n",
    "   .. py:method:: unassemble()\n",
    "\n",
    "      Forces the gp-model to re-assemble the covariance matrix on the next call.\n",
    "\n",
    "      :rtype: None\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "- {ref}`content:modules:gpr:1dexample`\n",
    "- {ref}`content:modules:gpr:2dexample`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv-fesslix)",
   "language": "python",
   "name": ".venv-fesslix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
